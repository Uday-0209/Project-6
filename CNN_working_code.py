# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fd_8XPbpYVyLwBnDlcMEJ8lJtxPue8C1
"""

# Commented out IPython magic to ensure Python compatibility.pip
import os
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.colors as colors
import scipy.signal as signal
from matplotlib import pyplot as plt
from tensorflow.keras.models import load_model
import pickle
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy


# %matplotlib inline
def stft_predict():
    data = tf.keras.utils.image_dataset_from_directory('D:\\Model\\data',batch_size=8)

    data_iterator = data.as_numpy_iterator()

    batch = data_iterator.next()

    data = data.map(lambda x,y: (x/255, y))

    data.as_numpy_iterator().next()

    train_size = int(len(data)*.7)
    val_size = int(len(data)*.2)
    test_size = int(len(data)*.1)+1

    train = data.take(train_size)
    val = data.skip(train_size).take(val_size)
    test = data.skip(train_size+val_size).take(test_size)

    model = Sequential()

    model.add(Conv2D(32, (3,3), 1, activation='relu', input_shape=(256,256,3)))
    model.add(MaxPooling2D())
    model.add(Conv2D(32, (3,3), 1, activation='relu'))
    model.add(MaxPooling2D())
    model.add(Conv2D(16, (3,3), 1, activation='relu'))
    model.add(MaxPooling2D())
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    model.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])

    logdir='D:\\Model\\logs'

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

    hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])

    pre = Precision()
    re = Recall()
    acc = BinaryAccuracy()

    for batch in test.as_numpy_iterator():
        X, y = batch
        yhat = model.predict(X)
        pre.update_state(y, yhat)
        re.update_state(y, yhat)
        acc.update_state(y, yhat)

    #model.save(os.path.join(model_file))
    with open('D:\Model\model_file.pkl', 'wb') as model_file:
        pickle.dump(model, model_file)

    def calcSTFT_norm(df, column_name, samplingFreq=10000, window='hann', nperseg=50000, figsize=(9, 5), cmap='plasma', ylim_max=None, save_path=None):
        inputSignal = df[column_name].values

        if len(inputSignal) < nperseg:
            print(f"Warning: nperseg = {nperseg} is greater than input length = {len(inputSignal)}. Skipping STFT calculation.")
            return

        f, t, Zxx = signal.stft(inputSignal, samplingFreq, window=window, nperseg=nperseg)

        fig = plt.figure(figsize=figsize)
        spec = plt.pcolormesh(t, f, np.abs(Zxx),
                              norm=colors.PowerNorm(gamma=1./6.),
                              cmap=plt.get_cmap(cmap))
        cbar = plt.colorbar(spec)

        ax = fig.axes[0]
        ax.set_xlim(0, t[-1])

        plt.title(f'STFT Spectrogram for {column_name}')
        ax.grid(True)
        ax.set_title('STFT Magnitude')
        if ylim_max:
            ax.set_ylim(0, ylim_max)
        ax.set_ylabel('Frequency [Hz]')
        ax.set_xlabel('Time [sec]')

        if save_path:
            plt.savefig(save_path)
            plt.show()


    file_name ='D:\Model\GD_1000_60_12_1.csv'
    df = pd.read_csv(file_name)

    column_names = ['cDAQ9189-20796A8Mod1_ai0', 'cDAQ9189-20796A8Mod1_ai1', 'cDAQ9189-20796A8Mod1_ai2', 'cDAQ9189-20796A8Mod1_ai3']

    for i, column_name in enumerate(column_names, start=1):
        base_save_path = f'output{i}'
        save_path = f'D:\model\{base_save_path}_stft.png'
        calcSTFT_norm(df, column_name, save_path=save_path)

    image_paths = [
        'D:\\Model\\output1_stft.png',
        'D:\\Model\\output2_stft.png',
        'D:\\Model\\output3_stft.png',
        'D:\\Model\\output4_stft.png'
    ]

    with open('D:\\Model\\model_file.pkl', 'rb') as modelfile:
        new_model = pickle.load(modelfile)

    results = []  # Store the results for each image

    for image_path in image_paths:
        img = cv2.imread(image_path)

        if img is not None:
            resize = tf.image.resize(img, (256, 256))

            if resize is not None:
                yhat = new_model.predict(np.expand_dims(resize / 255, 0))
                print(yhat)
                yhatt = float(yhat[0][0])
                print(yhatt)

                if yhatt > 0.5:
                    predicted_class = 'The data is Good'

                else:
                    predicted_class = 'The data is Bad'

                results.append(predicted_class)

    return results


# Call the function to get the results
results = stft_predict()

for predicted_class in results:
    print(f'The data  is {predicted_class}')